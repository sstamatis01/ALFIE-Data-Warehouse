import os
import asyncio
import json
import logging
from datetime import datetime
from aiokafka import AIOKafkaConsumer, AIOKafkaProducer
import requests
import pandas as pd
from io import BytesIO

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
logger = logging.getLogger("kafka_consumer_example")

KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
KAFKA_DATASET_TOPIC = os.getenv("KAFKA_DATASET_TOPIC", "dataset-events")
KAFKA_BIAS_TOPIC = os.getenv("KAFKA_BIAS_TOPIC", "bias-events")
KAFKA_AUTOML_TOPIC = os.getenv("KAFKA_AUTOML_TOPIC", "automl-events")
KAFKA_BIAS_TRIGGER_TOPIC = os.getenv("KAFKA_BIAS_TRIGGER_TOPIC", "bias-trigger-events")
KAFKA_AUTOML_TRIGGER_TOPIC = os.getenv("KAFKA_AUTOML_TRIGGER_TOPIC", "automl-trigger-events")
KAFKA_CONSUMER_GROUP = os.getenv("KAFKA_CONSUMER_GROUP", "dataset-consumer")
KAFKA_BIAS_CONSUMER_GROUP = os.getenv("KAFKA_BIAS_CONSUMER_GROUP", "bias-consumer")

API_BASE = os.getenv("API_BASE", "http://localhost:8000")


def fetch_dataset_metadata(user_id: str, dataset_id: str) -> dict:
    url = f"{API_BASE}/datasets/{user_id}/{dataset_id}"
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    return r.json()


def download_dataset_file(user_id: str, dataset_id: str) -> bytes:
    url = f"{API_BASE}/datasets/{user_id}/{dataset_id}/download"
    r = requests.get(url, timeout=60)
    r.raise_for_status()
    return r.content


# --- Consumer for dataset events ---
async def consume_dataset_events(producer):
    consumer = AIOKafkaConsumer(
        KAFKA_DATASET_TOPIC,
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        group_id=KAFKA_CONSUMER_GROUP,
        auto_offset_reset="earliest",
        enable_auto_commit=True,
        value_deserializer=lambda m: json.loads(m.decode("utf-8")),
        key_deserializer=lambda m: m.decode("utf-8") if m else None,
    )
    await consumer.start()
    logger.info(f"Started consumer for topic: {KAFKA_DATASET_TOPIC}")

    try:
        async for msg in consumer:
            key = msg.key
            value = msg.value
            logger.info(f"Message received from {msg.topic}: {json.dumps(value, indent=2)}")

            try:
                dataset = value.get("dataset", {})
                user_id = dataset.get("user_id")
                dataset_id = dataset.get("dataset_id")

                if not user_id or not dataset_id:
                    logger.warning("Missing user_id/dataset_id in event; skipping")
                    continue

                meta = fetch_dataset_metadata(user_id, dataset_id)
                file_bytes = download_dataset_file(user_id, dataset_id)

                try:
                    df = pd.read_csv(BytesIO(file_bytes))
                    logger.info(f"Loaded dataset with shape {df.shape}")
                except Exception as e:
                    logger.warning(f"Could not parse dataset as CSV: {e}")
                    df = None

                payload = {
                    "event_type": "bias-trigger.reported",
                    "dataset_id": dataset_id,
                    "user_id": user_id,
                    "target_column_name": "target",
                    "task_type": "classification",
                    "timestamp": datetime.utcnow().isoformat(),
                    "metadata": meta,
                    "record_count": len(df) if df is not None else None,
                }

                await producer.send_and_wait(
                    topic=KAFKA_BIAS_TRIGGER_TOPIC,
                    key=key,
                    value=payload,
                )
                logger.info(f"Produced bias trigger event to {KAFKA_BIAS_TRIGGER_TOPIC}")

            except Exception as e:
                logger.error(f"Error processing dataset event: {e}", exc_info=True)

    finally:
        await consumer.stop()
        logger.info("Dataset consumer stopped.")


# --- Consumer for bias events ---
async def consume_bias_events(producer):
    consumer = AIOKafkaConsumer(
        KAFKA_BIAS_TOPIC,
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        group_id=KAFKA_BIAS_CONSUMER_GROUP,
        auto_offset_reset="earliest",
        enable_auto_commit=True,
        value_deserializer=lambda m: json.loads(m.decode("utf-8")),
        key_deserializer=lambda m: m.decode("utf-8") if m else None,
    )
    await consumer.start()
    logger.info(f"Started consumer for topic: {KAFKA_BIAS_TOPIC}")

    try:
        async for msg in consumer:
            key = msg.key
            value = msg.value
            logger.info(f"Message received from {msg.topic}: {json.dumps(value, indent=2)}")
            try:
                user_id = value.get("user_id")
                dataset_id = value.get("dataset_id")
                target_column_name = value.get("target_column_name")
                task_type = value.get("task_type")

                # Here you would process the bias-event message
                # e.g., update status, trigger report generation, etc.
                # For now we just log it.
                logger.info(f"Processed bias event for dataset {value.get('dataset_id')}")
                #At this points we want to trigger the AutoML topic for training to start
                payload = {
                    "event_type": "automl-trigger.reported",
                    "dataset_id": dataset_id,
                    "user_id": user_id,
                    "target_column_name": target_column_name,
                    "task_type": task_type,
                    "time_budget": "10"
                    "timestamp": datetime.utcnow().isoformat(),
                    "metadata": meta,
                    "record_count": len(df) if df is not None else None,
                }
                await producer.send_and_wait(
                    topic=KAFKA_AUTOML_TRIGGER_TOPIC,
                    key=key,
                    value=payload,
                )
                logger.info(f"Produced AutoML trigger event to {KAFKA_AUTOML_TRIGGER_TOPIC}")
            except Exception as e:
                logger.error(f"Error processing dataset event: {e}", exc_info=True)

    finally:
        await consumer.stop()
        logger.info("Bias consumer stopped.")


# --- Consumer for automl events ---
async def consume_automl_events():
    consumer = AIOKafkaConsumer(
        KAFKA_AUTOML_TOPIC,
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        group_id=KAFKA_AUTOML_CONSUMER_GROUP,
        auto_offset_reset="earliest",
        enable_auto_commit=True,
        value_deserializer=lambda m: json.loads(m.decode("utf-8")),
        key_deserializer=lambda m: m.decode("utf-8") if m else None,
    )
    await consumer.start()
    logger.info(f"Started consumer for topic: {KAFKA_AUTOML_TOPIC}")

    try:
        async for msg in consumer:
            key = msg.key
            value = msg.value
            logger.info(f"Message received from {msg.topic}: {json.dumps(value, indent=2)}")

            # Here you would process the automl-event message
            # e.g., update status, trigger report generation, etc.
            # For now we just log it.
            logger.info(f"Processed bias event for dataset {value.get('dataset_id')}")

    finally:
        await consumer.stop()
        logger.info("Bias consumer stopped.")


# --- Main runner ---
async def run_consumers():
    producer = AIOKafkaProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        value_serializer=lambda x: json.dumps(x, default=str).encode("utf-8"),
        key_serializer=lambda x: x.encode("utf-8") if x else None,
    )

    await producer.start()
    logger.info("Producer started.")

    try:
        # Run both consumers concurrently
        await asyncio.gather(
            consume_dataset_events(producer),
            consume_bias_events(),
        )
    finally:
        await producer.stop()
        logger.info("Producer stopped.")


if __name__ == "__main__":
    try:
        asyncio.run(run_consumers())
    except KeyboardInterrupt:
        logger.info("Interrupted by user")
