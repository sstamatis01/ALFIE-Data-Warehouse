# All Kafka Consumers - Folder Support

## âœ… Implementation Complete

All Kafka consumer examples now support both single file and folder datasets with automatic unzip functionality!

---

## ðŸŽ¯ Updated Consumers

### 1. Agentic Core âœ…
**File**: `kafka_agentic_core_consumer_example.py`
- Checks `is_folder` in dataset events
- Extracts folder datasets
- Finds and loads CSV files
- Ready for multi-file processing

### 2. Bias Detector âœ…
**File**: `kafka_bias_detector_consumer_example.py`
- Checks `is_folder` in metadata
- Extracts folder datasets
- Analyzes first CSV file found
- Ready for multi-file bias detection

### 3. AutoML Consumer âœ…
**File**: `kafka_automl_consumer_example.py`
- Checks `is_folder` in dataset metadata
- Extracts folder datasets
- Uses first CSV file for training
- Ready for multi-file AutoML

### 4. XAI Consumer âœ…
**File**: `kafka_xai_consumer_example.py`
- Checks `is_folder` in dataset metadata
- Extracts folder datasets
- All files available for XAI analysis
- Ready for multi-file explainability

---

## ðŸ”§ What Each Consumer Does

### Pattern Used in All Consumers

```python
# 1. Fetch dataset metadata
metadata = fetch_dataset_metadata(user_id, dataset_id)
is_folder = metadata.get("is_folder", False)

# 2. Download dataset (file or ZIP)
file_bytes = download_dataset_file(user_id, dataset_id)

# 3. Handle based on type
if is_folder:
    # Extract ZIP
    extracted_files = extract_dataset_folder(file_bytes, f"temp_{dataset_id}")
    logger.info(f"Extracted {len(extracted_files)} files")
    
    # Find CSV files
    csv_files = [f for f in extracted_files if f.endswith('.csv')]
    
    # Process files
    for csv_file in csv_files:
        df = pd.read_csv(csv_file)
        # Your logic here
else:
    # Single file
    df = pd.read_csv(BytesIO(file_bytes))
    # Your logic here
```

---

## ðŸ“Š Consumer-Specific Handling

### Agentic Core
**Purpose**: Orchestrate pipeline, interact with user

**Folder Handling**:
- Extracts all files
- Lists all files for user
- Finds CSV for initial analysis
- TODO: Ask user which file(s) to process

**Example Output**:
```
Dataset type: FOLDER
File count: 3
Extracting folder dataset...
Extracted 3 files:
  - temp_dataset_test/train.csv
  - temp_dataset_test/test.csv
  - temp_dataset_test/metadata.json
Found 2 CSV file(s), loading first one for analysis
```

### Bias Detector
**Purpose**: Detect bias in data

**Folder Handling**:
- Extracts all files
- Finds CSV files
- Analyzes first CSV for bias
- TODO: Analyze all CSV files or combined dataset

**Example Output**:
```
Dataset type: FOLDER
File count: 3
Extracting folder dataset...
Extracted 3 files
Found 2 CSV file(s), loading first one for bias analysis
Loaded CSV with shape (1000, 10)
```

### AutoML Consumer
**Purpose**: Train ML models

**Folder Handling**:
- Extracts all files
- Finds CSV files for training
- Uses first CSV as training data
- TODO: Combine multiple CSVs or use train/test splits

**Example Output**:
```
Dataset type: FOLDER
File count: 3
Extracting folder dataset...
Extracted 3 files
Found 2 CSV file(s), loading first one for training
Dataset loaded: 1000 rows, 10 columns
Training model (using dummy model.pkl for testing)
```

### XAI Consumer
**Purpose**: Generate explainability reports

**Folder Handling**:
- Extracts all files
- All files available for analysis
- Can access any file for explanations
- TODO: Use specific files for different XAI methods

**Example Output**:
```
Dataset type: FOLDER
File count: 3
Extracting folder dataset...
Extracted 3 files:
  - temp_xai_test/train.csv
  - temp_xai_test/test.csv
  - temp_xai_test/metadata.json
NOTE: All extracted files are available for XAI analysis
```

---

## ðŸ”„ Complete Flow with Folders

```
1. User uploads folder (3 CSV files)
   â†“
2. DW: is_folder=true, file_count=3
   â†“
3. Kafka: dataset-events with is_folder=true
   â†“
4. Agentic Core: Downloads ZIP â†’ Extracts â†’ Finds CSVs
   â†“
5. Agentic Core: Produces bias-trigger
   â†“
6. Bias Detector: Downloads ZIP â†’ Extracts â†’ Analyzes CSV
   â†“
7. Bias Detector: Posts report â†’ Kafka bias-events
   â†“
8. Agentic Core: Produces automl-trigger
   â†“
9. AutoML: Downloads ZIP â†’ Extracts â†’ Trains on CSV
   â†“
10. AutoML: Uploads model â†’ Kafka automl-events
    â†“
11. Agentic Core: Produces xai-trigger
    â†“
12. XAI: Downloads ZIP â†’ Extracts â†’ All files available
    â†“
13. XAI: Generates reports â†’ Kafka xai-events
    â†“
14. Agentic Core: ML PIPELINE COMPLETED! âœ“
```

---

## ðŸ’» Code Example - Processing Multiple Files

### Example: Process All CSV Files in Folder

```python
# In any consumer
if is_folder:
    extracted_files = extract_dataset_folder(file_bytes, "temp_dir")
    
    # Find all CSV files
    csv_files = [f for f in extracted_files if f.endswith('.csv')]
    
    # Process each CSV
    for csv_file in csv_files:
        logger.info(f"Processing: {csv_file}")
        df = pd.read_csv(csv_file)
        
        # Your processing logic here
        # Example: Bias detection on each file
        # Example: Train separate models
        # Example: Generate separate XAI reports
```

### Example: Combine Multiple CSV Files

```python
# In AutoML consumer - combine train/test splits
if is_folder:
    extracted_files = extract_dataset_folder(file_bytes, "temp_dir")
    csv_files = [f for f in extracted_files if f.endswith('.csv')]
    
    # Find train/test files
    train_file = next((f for f in csv_files if 'train' in f.lower()), None)
    test_file = next((f for f in csv_files if 'test' in f.lower()), None)
    
    if train_file and test_file:
        train_df = pd.read_csv(train_file)
        test_df = pd.read_csv(test_file)
        
        logger.info(f"Train: {train_df.shape}, Test: {test_df.shape}")
        # Train with proper train/test split
    else:
        # Fallback: use first CSV
        df = pd.read_csv(csv_files[0])
```

---

## ðŸ§ª Testing All Consumers

### Step 1: Upload Folder Dataset
```bash
# Create test folder
zip -r test_folder.zip train.csv test.csv metadata.json

# Upload
curl -X POST "http://localhost:8000/datasets/upload/folder/testuser" \
  -F "zip_file=@test_folder.zip" \
  -F "dataset_id=folder-flow-test" \
  -F "name=Folder Flow Test"
```

### Step 2: Watch Consumer Logs

**Agentic Core (Terminal 3):**
```
[Dataset Event] Message received
Dataset type: FOLDER
File count: 3
Extracting folder dataset...
Extracted 3 files:
  - temp_dataset_folder-flow-test/train.csv
  - temp_dataset_folder-flow-test/test.csv
  - temp_dataset_folder-flow-test/metadata.json
Found 2 CSV file(s), loading first one for analysis
```

**Bias Detector (Terminal 4):**
```
Dataset type: FOLDER
File count: 3
Extracting folder dataset...
Extracted 3 files
Found 2 CSV file(s), loading first one for bias analysis
```

**AutoML Consumer (Terminal 5):**
```
Dataset type: FOLDER
File count: 3
Extracting folder dataset...
Extracted 3 files
Found 2 CSV file(s), loading first one for training
Dataset loaded: 100 rows, 5 columns
```

**XAI Consumer (Terminal 6):**
```
Dataset type: FOLDER
File count: 3
Extracting folder dataset...
Extracted 3 files
NOTE: All extracted files are available for XAI analysis
```

---

## ðŸ“‹ Summary of Changes

### All 4 Consumers Updated

| Consumer | Added extract_dataset_folder() | Checks is_folder | Extracts ZIP | Processes Files |
|----------|-------------------------------|------------------|--------------|-----------------|
| **Agentic Core** | âœ… | âœ… | âœ… | âœ… |
| **Bias Detector** | âœ… | âœ… | âœ… | âœ… |
| **AutoML** | âœ… | âœ… | âœ… | âœ… |
| **XAI** | âœ… | âœ… | âœ… | âœ… |

### Common Features

All consumers now:
- âœ… Have `extract_dataset_folder()` helper function
- âœ… Check `is_folder` field from metadata
- âœ… Download dataset (automatically gets ZIP for folders)
- âœ… Extract ZIP if folder
- âœ… List all extracted files
- âœ… Find CSV files for processing
- âœ… Include example code for multi-file processing

---

## ðŸŽ‰ Complete Implementation

**Backend API:**
- âœ… Upload folder endpoint
- âœ… Download with file selection
- âœ… Delete folder properly
- âœ… List files endpoint
- âœ… Kafka messages with is_folder

**All Consumer Scripts:**
- âœ… Agentic Core
- âœ… Bias Detector
- âœ… AutoML Consumer
- âœ… XAI Consumer

**Features:**
- âœ… Automatic folder detection
- âœ… Automatic ZIP extraction
- âœ… File discovery
- âœ… Example processing code
- âœ… Backward compatible with single files

**Documentation:**
- âœ… API documentation
- âœ… Kafka integration guide
- âœ… Consumer updates documented
- âœ… Testing guide

---

## ðŸš€ Ready to Test

**Restart all services:**
```bash
# Terminal 1: API
python run.py

# Terminal 2-5: Consumers
python kafka_agentic_core_consumer_example.py
python kafka_bias_detector_consumer_example.py
python kafka_automl_consumer_example.py
python kafka_xai_consumer_example.py
```

**Test with folder:**
```bash
zip -r test.zip file1.csv file2.csv
curl -X POST "http://localhost:8000/datasets/upload/folder/testuser" \
  -F "zip_file=@test.zip" \
  -F "dataset_id=test-folder" \
  -F "name=Test Folder"
```

**Watch all 4 consumers extract and process the files!** ðŸŽŠ

The complete system now supports folders across the entire pipeline! ðŸš€

