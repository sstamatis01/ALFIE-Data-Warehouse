# Sharing Code with Partners - Key Information

## üéØ Your Question Answered

**Q: Will partners running in separate Docker containers be able to reach Kafka, MongoDB, MinIO?**

**A: YES, but with important considerations:**

### ‚úÖ What Works (Your Current Setup)
- You run DW services in Docker
- You run consumer scripts OUTSIDE Docker (on host machine)
- You use `localhost:9092` for Kafka, `localhost:8000` for API
- **This works because ports are exposed to host**

### ‚ö†Ô∏è What Changes (Partner in Docker)
- Partner runs their service IN Docker (separate container)
- **`localhost` won't work** - it refers to the container itself
- Must use **Docker service names** or **host networking**

---

## üîë Key Points for Partners

### 1. Network Configuration

**Option A: Docker Compose Network (Recommended)**
```yaml
# Partner's docker-compose.yml
services:
  agentic-core:
    image: partner/service:latest
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092     # ‚Üê Service name!
      API_BASE: http://dw-api:8000             # ‚Üê Service name!
    networks:
      - data-warehouse-network  # ‚Üê Join your network
```

**Option B: Host Networking (Simpler)**
```yaml
services:
  agentic-core:
    network_mode: "host"
    environment:
      KAFKA_BOOTSTRAP_SERVERS: localhost:9092  # ‚Üê localhost works!
      API_BASE: http://localhost:8000          # ‚Üê localhost works!
```

### 2. What Partners CAN Access

‚úÖ **Kafka** - YES (needed for event-driven architecture)
- `kafka:29092` (inside Docker) or `localhost:9092` (outside Docker)

‚úÖ **DW API** - YES (needed for data upload/download)
- `http://dw-api:8000` (inside Docker) or `http://localhost:8000` (outside Docker)

‚ùå **MongoDB** - NO (should use API instead)
- Direct access not recommended for security

‚ùå **MinIO** - NO (should use API instead)
- Direct access not recommended for security

---

## üì¶ Files to Share with Partners

### Core Files
- ‚úÖ `kafka_agentic_core_consumer_example.py` - Orchestrator template
- ‚úÖ `kafka_bias_detector_consumer_example.py` - Bias detector template
- ‚úÖ `kafka_automl_consumer_example.py` - AutoML template
- ‚úÖ `kafka_xai_consumer_example.py` - XAI template

### Configuration
- ‚úÖ `env.example.txt` - Environment variables template
- ‚úÖ `docker-compose.partner-example.yml` - Docker compose example

### Documentation
- ‚úÖ `DOCKER_DEPLOYMENT_GUIDE.md` - Complete networking guide
- ‚úÖ `KAFKA_ORCHESTRATION_COMPLETE.md` - Complete Kafka flow
- ‚úÖ `KAFKA_QUICK_REFERENCE.md` - Quick reference
- ‚úÖ `API_CHANGES_USER_ID_AND_VERSIONING.md` - API documentation
- ‚úÖ `PARTNER_INTEGRATION_CHECKLIST.md` - Integration checklist

### Optional (for reference)
- ‚úÖ `TEST_COMPLETE_FLOW.md` - Testing guide
- ‚úÖ `IMPLEMENTATION_SUMMARY.md` - Implementation details

---

## üöÄ Quick Start for Partners

### Step 1: Clone/Receive Code
```bash
# Partners receive the code package
git clone <your-repo>
cd data-warehouse-app
```

### Step 2: Set Up Environment
```bash
# Copy environment template
cp env.example.txt .env

# Edit with correct values
# For Docker: kafka:29092, http://dw-api:8000
# For local: localhost:9092, http://localhost:8000
nano .env
```

### Step 3: Choose Deployment Method

**Method A: Run on Host (Development)**
```bash
# Just run the script - uses localhost
python kafka_agentic_core_consumer_example.py
```

**Method B: Run in Docker (Production)**
```bash
# Build container
docker build -t partner/agentic-core .

# Run with network
docker run --network data-warehouse-network \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:29092 \
  -e API_BASE=http://dw-api:8000 \
  partner/agentic-core
```

---

## üîí Security Best Practices

### What Partners Should Do
‚úÖ **Access DW only via API** - Don't directly connect to MongoDB/MinIO
‚úÖ **Use dedicated Kafka consumer groups** - Don't conflict with other services
‚úÖ **Handle errors gracefully** - Network issues, Kafka downtime, etc.
‚úÖ **Use environment variables** - Don't hardcode connection strings
‚úÖ **Implement retries** - For transient failures

### What Partners Should NOT Do
‚ùå **Don't directly access MongoDB** - Use API endpoints
‚ùå **Don't directly access MinIO** - Use API for file download
‚ùå **Don't hardcode hostnames** - Use environment variables
‚ùå **Don't expose sensitive data** - In logs or error messages
‚ùå **Don't skip error handling** - Services will fail

---

## üìã Pre-Integration Checklist

Before partners start:

- [ ] **Understand the flow** - Read `KAFKA_ORCHESTRATION_COMPLETE.md`
- [ ] **Test connectivity** - Can reach Kafka and API
- [ ] **Review examples** - Understand consumer scripts
- [ ] **Configure environment** - Set up `.env` correctly
- [ ] **Test on host first** - Run scripts outside Docker
- [ ] **Then containerize** - Move to Docker after it works

---

## üß™ Testing Connectivity

### Test 1: From Host Machine
```bash
# Test Kafka
nc -zv localhost 9092

# Test API
curl http://localhost:8000/health

# Run consumer
python kafka_agentic_core_consumer_example.py
```

### Test 2: From Docker Container
```bash
# Test Kafka
docker run --network data-warehouse-network nicolaka/netshoot \
  nc -zv kafka 29092

# Test API
docker run --network data-warehouse-network nicolaka/netshoot \
  curl http://dw-api:8000/health
```

---

## üí° Common Questions

### Q: Do we need to install MongoDB/MinIO?
**A: NO!** You provide these services. Partners only need:
- Kafka client library (aiokafka)
- HTTP client (requests)
- Their business logic

### Q: Can partners modify the DW code?
**A: NO!** DW code is yours. Partners only:
- Use the consumer examples as templates
- Implement their own services
- Call your API endpoints

### Q: What if partners want direct database access?
**A: Discourage it!** Direct access means:
- Security risks
- No API audit trail
- Tight coupling
- Harder to upgrade

If absolutely necessary, document that it's NOT recommended.

### Q: What ports need to be exposed?
**A: Only these:**
- Kafka: 9092 (for external access)
- DW API: 8000 (for external access)
- MongoDB: Keep INTERNAL only
- MinIO: Keep INTERNAL only

---

## üìä Current vs Partner Architecture

### Current (Your Development Setup)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Docker (Your Machine)              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇKafka ‚îÇ ‚îÇMongo ‚îÇ ‚îÇ DW API‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ     ‚îÇ                   ‚îÇ           ‚îÇ
‚îÇ     ‚îÇ Ports exposed     ‚îÇ           ‚îÇ
‚îÇ     ‚îÇ :9092, :8000      ‚îÇ           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     ‚îÇ  Host Machine     ‚îÇ           ‚îÇ
‚îÇ     ‚îÇ  (Your Terminal)  ‚îÇ           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  Consumer Scripts           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Uses localhost:9092      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Uses localhost:8000      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Partner Setup (Production)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Docker Network: data-warehouse-network ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇKafka ‚îÇ ‚îÇMongo ‚îÇ ‚îÇ DW API‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ     ‚îÇ                   ‚îÇ              ‚îÇ
‚îÇ     ‚îÇ Internal network  ‚îÇ              ‚îÇ
‚îÇ     ‚îÇ kafka:29092       ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     ‚îÇ  Partner Container‚îÇ              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Partner Service            ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - Uses kafka:29092         ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  - Uses dw-api:8000         ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚úÖ Summary

**Your Question: Can partners reach Kafka, MongoDB, MinIO from separate Docker containers?**

**Answer:**

1. **Kafka: YES** ‚úÖ
   - Use `kafka:29092` (inside Docker) or `localhost:9092` (outside Docker)
   - Needed for event-driven architecture

2. **DW API: YES** ‚úÖ
   - Use `http://dw-api:8000` (inside Docker) or `http://localhost:8000` (outside Docker)
   - Needed for data upload/download

3. **MongoDB: NOT RECOMMENDED** ‚ö†Ô∏è
   - CAN access if on same network: `mongodb:27017`
   - But SHOULD use API instead for security

4. **MinIO: NOT RECOMMENDED** ‚ö†Ô∏è
   - CAN access if on same network: `minio:9000`
   - But SHOULD use API instead for security

**Key Point:** Your current setup (running scripts on host) works perfectly! Partners will need to adjust hostnames when containerizing, but the logic stays the same.

---

## üì¶ Package to Share

Create a partner package:

```bash
# Create partner package directory
mkdir data-warehouse-partner-package
cd data-warehouse-partner-package

# Copy essential files
cp kafka_*_consumer_example.py .
cp env.example.txt .
cp docker-compose.partner-example.yml .
cp *_README.md .
cp *_GUIDE.md .
cp PARTNER_INTEGRATION_CHECKLIST.md .

# Create README
cat > README.md << 'EOF'
# Data Warehouse Integration Package

## Quick Start
1. Read DOCKER_DEPLOYMENT_GUIDE.md
2. Copy env.example.txt to .env
3. Configure your environment
4. Test consumer scripts
5. Containerize when ready

## Support
Contact: your-email@example.com
EOF

# Create archive
tar -czf data-warehouse-partner-package.tar.gz .
```

---

## üéâ You're Ready!

Everything partners need is documented and ready to share. The key points:

‚úÖ Consumer scripts work as-is (just change hostnames for Docker)
‚úÖ Network configuration is well-documented
‚úÖ Security best practices included
‚úÖ Testing guides provided
‚úÖ Complete examples available

**Your current setup proves it works - partners just need to adjust for Docker networking!** üöÄ

